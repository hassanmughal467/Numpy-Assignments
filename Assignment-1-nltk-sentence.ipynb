{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download \n",
    "#import nltk\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOrd Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forever',\n",
       " 'alone',\n",
       " 'in',\n",
       " 'a',\n",
       " 'crowd',\n",
       " ',',\n",
       " 'failed',\n",
       " 'comedian',\n",
       " 'Arthur',\n",
       " 'Fleck',\n",
       " 'seeks',\n",
       " 'connection',\n",
       " 'as',\n",
       " 'he',\n",
       " 'walks',\n",
       " 'the',\n",
       " 'streets',\n",
       " 'of',\n",
       " 'Gotham',\n",
       " 'City.Arthur',\n",
       " 'wears',\n",
       " 'two',\n",
       " 'masks',\n",
       " '--',\n",
       " 'the',\n",
       " 'one',\n",
       " 'he',\n",
       " 'paints',\n",
       " 'for',\n",
       " 'his',\n",
       " 'day',\n",
       " 'job',\n",
       " 'as',\n",
       " 'a',\n",
       " 'clown',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'guise',\n",
       " 'he',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'a',\n",
       " 'futile',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'around',\n",
       " 'him',\n",
       " '.',\n",
       " 'Isolated',\n",
       " ',',\n",
       " 'bullied',\n",
       " 'and',\n",
       " 'disregarded',\n",
       " 'by',\n",
       " 'society',\n",
       " ',',\n",
       " 'Fleck',\n",
       " 'begins',\n",
       " 'a',\n",
       " 'slow',\n",
       " 'descent',\n",
       " 'intomadness',\n",
       " 'as',\n",
       " 'he',\n",
       " 'transforms',\n",
       " 'into',\n",
       " 'the',\n",
       " 'criminal',\n",
       " 'mastermind',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Joker',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite = \"Forever alone in a crowd, failed comedian Arthur Fleck seeks connection as he walks the streets of Gotham City.Arthur wears two masks -- the one he paints for his day job as a clown, and the guise he projects in a futile attempt to feel like he's part of the world around him. Isolated, bullied and disregarded by society, Fleck begins a slow descent intomadness as he transforms into the criminal mastermind known as the Joker.\"\n",
    "\n",
    "word_tokenize(kite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parag = \"Forever alone in a crowd, failed comedian Arthur Fleck seeks connection as he walks the streets of Gotham City. \\\n",
    "Arthur wears two masks -- the one he paints, for his day job as a clown ! and the guise he projects in a futile attempt to\\\n",
    "feel like he's part of the world around him. Isolated, bullied and disregarded by society? Fleck begins a slow descent into\\\n",
    "madness as he transforms into the criminal mastermind known as the Joker.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Forever alone in a crowd, failed comedian Arthur Fleck seeks connection as he walks the streets of Gotham City.',\n",
       " 'Arthur wears two masks -- the one he paints, for his day job as a clown !',\n",
       " \"and the guise he projects in a futile attempt tofeel like he's part of the world around him.\",\n",
       " 'Isolated, bullied and disregarded by society?',\n",
       " 'Fleck begins a slow descent intomadness as he transforms into the criminal mastermind known as the Joker.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myArr = sent_tokenize(parag)\n",
    "myArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['Forever alone in a crowd\",\n",
       " \" failed comedian Arthur Fleck seeks connection as he walks the streets of Gotham City.'\",\n",
       " \" 'Arthur wears two masks -- the one he paints\",\n",
       " \" for his day job as a clown !'\",\n",
       " ' \"and the guise he projects in a futile attempt tofeel like he\\'s part of the world around him.\"',\n",
       " \" 'Isolated\",\n",
       " \" bullied and disregarded by society?'\",\n",
       " \" 'Fleck begins a slow descent intomadness as he transforms into the criminal mastermind known as the Joker.']\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=str(myArr)\n",
    "s.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming is a kind of normalization for words. The words which have the same meaning but have some variation according \n",
    "#to the context or sentence are normalized.\n",
    "#He was riding.\t\n",
    "#He was taking the ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "guru99\n",
      ",\n",
      "you\n",
      "have\n",
      "to\n",
      "build\n",
      "a\n",
      "veri\n",
      "good\n",
      "site\n",
      "and\n",
      "I\n",
      "love\n",
      "visit\n",
      "your\n",
      "site\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hello Guru99, You have to build a very good site and I love visiting your site.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "\trootWord=ps.stem(w)\n",
    "\tprint(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Stemming is a data-preprocessing module. The English language has many variations of a single word. \\nThese variations create ambiguity in machine learning training and prediction. To create a successful model,\\nit's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an \\nimportant technique to get row data from a set of sentence and removal \\nof redundant data also known as normalization.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Stemming is a data-preprocessing module. The English language has many variations of a single word. \n",
    "These variations create ambiguity in machine learning training and prediction. To create a successful model,\n",
    "it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an \n",
    "important technique to get row data from a set of sentence and removal \n",
    "of redundant data also known as normalization.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning.It helps in returning\n",
    "###### the base or dictionary form of a word, which is known as the lemma.The NLTK Lemmatization method is based on\n",
    "###### WorldNet's built-in morph function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is Lemmatization better than Stemming?\n",
    "###### Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "\n",
    "###### On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra RegExp tokenizer Instead word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "too = \"I won't let you do what came here to do so you shan't do that, we wont let you do it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'what',\n",
       " 'came',\n",
       " 'here',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'you',\n",
       " 'sha',\n",
       " \"n't\",\n",
       " 'do',\n",
       " 'that',\n",
       " ',',\n",
       " 'we',\n",
       " 'wont',\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'it']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it is storing won't as a different word but if we want to store won't as a single word we use regexp_tokenize\n",
    "word_tokenize(too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"won't\",\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'what',\n",
       " 'came',\n",
       " 'here',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'you',\n",
       " \"shan't\",\n",
       " 'do',\n",
       " 'that',\n",
       " 'we',\n",
       " 'wont',\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'it']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(too, \"[\\w']+\")     # Now it is using won't as a single word above it was using won't as a different word\n",
    "                                   #also \"[\\w']\" it will separate words including wont as a singular word if we dont use ' then it will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another one also used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"won't\",\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'what',\n",
       " 'came',\n",
       " 'here',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'you',\n",
       " \"shan't\",\n",
       " 'do',\n",
       " 'that',\n",
       " 'we',\n",
       " 'wont',\n",
       " 'let',\n",
       " 'you',\n",
       " 'do',\n",
       " 'it']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg = RegexpTokenizer(\"[\\w']+\")           #it Also can be done like this\n",
    "reg.tokenize(too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\REHMAN COMPUTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk php/NN)\n",
      "  from/IN\n",
      "  (mychunk guru99/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class nltk.tokenize.casual.TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)[source]\\nBases: object'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class nltk.tokenize.casual.TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)[source]\n",
    "Bases: object'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "tknzr.tokenize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
